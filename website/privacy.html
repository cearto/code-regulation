<!DOCTYPE HTML>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"><![endif]-->
<!--[if IE 7]><html class="no-js lt-ie9 lt-ie8"><![endif]-->
<!--[if IE 8]><html class="no-js lt-ie9"><![endif]-->
<!--[if gt IE 8]><!--><html class="no-js"><!--<![endif]-->
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
	<meta name="viewport" content="width=device-width, initial-scale = 1.0, user-scalable = no">
	<title>Code Regulation - CS181 Project</title>
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700,600' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" href="css/normalize.css" type="text/css" media="screen">
	<link rel="stylesheet" href="css/grid.css" type="text/css" media="screen">
	<link rel="stylesheet" href="css/style.css" type="text/css" media="screen">
	<!-- <link rel="stylesheet" href="css/style.min.css" type="text/css" media="screen"> -->
	<!--[if IE]><script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
</head>

<body>

	<div class="menu">
		<div class="container clearfix">

			<div id="logo" class="grid_3">
			<h1> Code & Regulation </h1>
			<!--	<img src="images/logo.png">-->
			</div>

			<div id="nav" class="grid_9 omega">
				<ul class="navigation">
					
					<li data-slide="1"> Overview</li>
					<li data-slide="2"> Privacy in Cyberspace </li>
					<li data-slide="3"> Case studies </li>
					<li data-slide="4"> Ethics </li>
				</ul>
			</div>

		</div>
	</div>


	<div class="slide" id="slide1" data-slide="1" data-stellar-background-ratio="0.5">
		<div class="container clearfix">

			<div id="content" class="grid_7">
				<aside id='breadcrumbs'> <a href='index.html'>Home </a> > Privacy </aside>
				<h1>Privacy</h1>
 
		<h2>Privacy on the web</h2>
		 
		<p>In contrast to <a href="translation.html">translation</a> and <a href="/ip.html">intellectual property</a>, code has already upset the traditional balance of our expectations as far as privacy is concerned. So the question is: could code recreate the traditional balance we enjoyed prior to porting our lives online? According to Lessig, it can, and it should.</p>
		 
		<p>What is privacy? As defined by Ethan Katsch, <span>it is the power to control what others can come to know about you</span>.<a class = "cit" href="#ref-1">[1]</a> Lessig claims that there are two ways it can be impinged upon: via monitoring, and via searching. Monitoring is the ability to observe and react to your public behavior. Searching, on the other hand, involves learning information about someone through the records their behavior leads behind. Before cyberspace was widely occupied, monitoring took place</p>
		 
		<p>Now, however, both monitoring and searching of cyber behavior have become easier than ever. Take email for example. Not only is it monitorable, archivable, and searchable, but it is inexpensive and generally unobtrusive to do so. Given the ease and unnoticeable nature of monitoring and searching email, and other similarly monitorable and searchable web activities, does doing so impinge upon our constitutional right to privacy? And if so, how can we guarantee that our right to privacy is respected?</p>
			</div>
			<div id="decorative" class="grid_5 omega">
				<img src="http://www.hideyourselfonline.com/blog/wp-content/uploads/2011/06/internet-privacy.jpg">
			</div>

		</div>
	</div>



	<div class="slide" id="slide2" data-slide="2" data-stellar-background-ratio="0.5">
		<div class="container clearfix">

			<div id="content" class="grid_12">
				<h2>Conceptions of privacy</h2>
 
				<p>Lessig defines three conceptions of privacy: privacy as placing a minimal burden on the searched or monitored party, privacy as respecting dignity, and privacy as substantive in and of itself. Because of latent constitutional ambiguity about which capacity our right to privacy is protected within, the different interpretations of privacy could lead to drastically different legislation with regard to privacy on the net.</p>
				 
				<p>The conception of privacy that gives the greatest import to minimized burden argues that the point of constitutional privacy protections was to minimize intrusion. When interventions made less burdensome, protection against it decreases as well. Under this conception, noninvasive email monitoring or unnoticeable searching by a government worm that only returns information about illegal activities would not violate constitutional rights. Another conception of privacy views it as a manner of respecting dignity. That is, the very idea of a search is harmful to dignity. There is something insulting about being searched, even if it is "reasonable." Here, search of monitored web activity by a government is construed as an affront to dignity. Finally, a third conception views privacy itself as substantive. In other words, it is a way to constrain the power of the state to regulate—a substantive limit on the government's power, limiting the kind of regulation the government can effectively impose. These three conceptions could yield different results for how we deal with the fact that the government could now conduct perfectly noninvasive searches.</p>
				
				<h2>Why protect privacy?</h2>
				 
				<p>Much information about your activity on the internet is technically public. So what could be wrong with collecting information that is publicly displayed? On the one hand, we might argue that there is nothing wrong with it. When you act in public, you give up any right to privacy—those actions can be monitored in the public domain, and if there are records of your public activity, they can also be searched. Moreover, the harm is not actually that great. As long as you aren’t breaking the law, you shouldn’t be punished for your actions. Finally, allowing your data to be monitored could actually prove useful—for example, allowing Google to search through your emails allows them to show you ads that are relevant to your interests.</p>
				 
				<p>On the other hand, values that were originally protected by the imperfection of monitoring technology helped preserve important substantive values. When all of your activity is monitored and searchable, the burden is on the monitored to establish their innocence and assure all who see ambiguous facts of your innocence. Additionally, privacy enables multiple communities, disabling the power of one dominant community to norm others into oblivion. Additionally, the personalized nature of ads based on data collection also has a downside. It arguably enables discrimination, which could result in poorer offerings for "less desirable" customers.</p>
				 
				<p>Thus, in legislating an architecture of cyberspace that allows for a desirable level of privacy to be preserved, we must consider which conception of privacy to adhere to, and how the above mentioned costs and benefits apply differently in each case.</p>
				 
				<h2>Lessig’s proposed solution</h2>
				 
				<p>Lessig proposes solutions for how might we act to restore control over personal data that are collected and searchable by the architectures of cyberspace. His solutions involve legally altering the architecture of cyberspace to grant its members more control over their private information.</p>
				 
				<p>Lessig’s solution to the problem posed by searchability of cyberspace is to allow for communication performed over various services to be encrypted, and thus unsearchable by the service provider or anyone else with access to the information. Additionally, Lessig argues that inoffensive or invisible searches should be banned. This way, even if our information is searchable, we could at least be aware of which information is searchable and who has access to it.</p>
				 
				<p>Lessig’s solution to the problem of monitoring is slightly more aggressive. He argues that the government ought to create a machine-to-machine protocol for negotiating privacy protections. This would allow for users to negotiate which aspects of their information they consent to have monitored. Privacy preferences would be entered into the machine, and the machine would then perform negotiations on behalf of the user with services the user subscribes to. Users could thus also be more informed about what information about their online activity was being monitored. This would prevent fiascos such as that which occurred when Facebook instituted Newsfeed, which is discussed in the case studies section below.</p>
				 
				<h2>Critique of Lessig’s Solution</h2>
				 
				<p>Paul Schwartz has criticized Lessig’s idea of construing privacy as property, arguing instead an approach to internet privacy centered on fair information practices (FIP’s).<a class = "cit" href="#ref-2">[2]</a> Schwartz argues that the P3P interface for controlling personal information settings that would be a part of a protocol such as the one Lessig describes would likely be just as difficult to understand and navigate as the privacy agreements we routinely sign without reading. Moreover, he argues, not all personal data should belong to the individual. Certain information about individuals is necessarily external to the individual, and actually belongs in the public sphere. For example, government agencies often require access to personal information in order to determine eligibility for public benefits. Additionally, Schwartz argues, a democratic society requires public accountability. If an individual has complete control over their information, the public cannot access the information to provide an adequate critical assessment.</p>
				 
				<p>In place of Lessig’s property proposal, Schwartz recommends the use of FIPs to regulate internet privacy. Instead of negotiating exchanges of information and services, FIPs render service providers liable if they use information in a way that is harmful for the person it belongs to. FIPs require service providers to self-regulate to a greater extent. Because of the liability introduced in failure to meet a given set of standards, such a policy would incentivize improved understanding and control of privacy if desired by the user. A Platform for Privacy Preferences (P3P) type interface could also develop within this framework, but the instituting organization would have a greater incentive to make the interface easy to understand. However, FIPs are dependent on self-regulation—as such, in contrast with the solution Lessig has proposed, they may only impact the aspects of privacy that are likely to cause noticeable damage if breached. Thus, under Schwartz’s framework, we may not be protected from unnoticeable or noninvasive search and monitoring.</p>
			</div>

		</div>
	</div>



	<div class="slide" id="slide3" data-slide="3" data-stellar-background-ratio="0.5">
		<div class="container clearfix">

			<div id="content" class="grid_12">
				<h2>Recent case studies</h2>
 
				<p>In 2006, Facebook launched its News Feed feature, and thousands of users expressed discontent at the manner in which the feature made easily visible to friends all activity that users performed on the site.<a class = "cit" href="#ref-3">[3]</a> While the activity could previously have been found by friends with a greater amount of effort, the new feature made it easy for everyone to monitor their friends’ activity. Danah Boyd argues that the ingredient that made this change so intolerable was social convergence—collapsing disparate social contexts into one. In effect, it threatens people’s ability to maintain different communities and different types of relationships with different people. Since the outcry, Facebook has made it easier for users to control who sees what, which limits social convergence. However, users are still vulnerable for their information to be exposed, as many are generally uninformed about which aspects of their behavior can be monitored and searched by their friends, Facebook, or even outside organizations. Thus, the problem that Lessig has brought up in which users of web services have lost the ability to easily control their private information remains an issue.</p>
				 
				<p>While Google has been subject to many privacy concerns, particularly with regard to its data retention intervals for its search service, the privacy design of Gmail has been considered a success.<a class = "cit" href="#ref-4">[4]</a> Gmail offered its users a clear explanation of its reasons for showing ads, and managed to show these ads while avoiding user profiling and disclosure of private user information to advertisers. In 2012, however, when Google altered its privacy policy so that the policies that previously existed across 60 different products were now combined into one policy, users lost a great deal of control over which Google services had access to their private information. While Google was transparent about the policy change, the all-or-nothing approach Google took here was a design choice that was detrimental to its users’ ability to control privacy. Thus, while Google adhered to FIPs by being clear about their policy change, the architecture of their privacy design was problematic.</p>
				 
				<p>Rubinstein and Good claim that FIPs can provide greater protection than they currently do, but that in order for this to be the case, they must be translated into certain principles of engineering design. FIPs would thus specify engineering design principles as well as more abstract privacy principles. This could alleviate the aforementioned concern that FIPs do not provide complete enough protection. It could thus prevent detrimental design choices as in the case of Google’s combined privacy policy. However, while Lessig’s proposal may be overly aggressive, it would ensure the greatest amount of control in cases such as these.</p>
			</div>

			

		</div>
	</div>



	<div class="slide" id="slide4" data-slide="4" data-stellar-background-ratio="0.5">
		<div class="container clearfix">

			<div id="content" class="grid_12">			
				<h2>Ethical Considerations</h2>

				<p>A deontological evaluation of regulation of privacy on the net might agree with Lessig in that the government should institute rules allowing users to control which of their information can be monitored and searched. If companies are given rules about how to respect privacy, they can fulfill their duty to users in only using their information as expected. However, from a utilitarian standpoint, privacy regulation might prevent companies from using your information to improve their services. If services can use private information to better tailor ads and services, then even though certain expectations about privacy may be violated, overall happiness can be increased. Thus, a utilitarian would likely support a more laissez-faire, less regulated approach to privacy in cyberspace.
</p>

			</div>

		</div>
	</div>
	<div class="slide" id="slide5" data-slide="5" data-stellar-background-ratio="0.5">
		<div class="container clearfix">

			<div id="content" class="grid_12">			
				<h2>Related Links & Citations</h2>
					<p><a href=”https://www.cdt.org/issue/consumer-privacy”>Center for Democracy and Technology’s News, Research, and Analysis of Consumer Privacy</a></p>
<p><a href=”https://ilt.eff.org/index.php/Privacy:_Government_Regulation”>Electronic Frontier Foundation’s Summary of Current Government Privacy Regulation</a></p>
<p><a href=”http://cylaw.info/internet-governance-2/”>Cyberlaw Internet regulatory models for cyberspace</a></p>
<p><a href=”http://www.infoworld.com/d/security/groups-criticize-fbi-plan-require-internet-backdoors-wiretaps-217706”>FBI plans to require internet backdoors for wiretapping</a></p>
					<hr/>
					<div class="citations">
					<span id="ref-1">
					<em>[1]</em>Lessig, Lawrence. Code and Other Laws of Cyberspace. New York: Basic, 1999. Print.
				</span>
					<span id="ref-2"><em>[2]</em> Schwartz, Paul M. "Beyond Lessig's Code for Internet Privacy: Cyberspace Filters, Privacy Control, and Fair Information Practices." Wis. L. Rev. (2000): 743. </span>
					<span id="ref-3"><em>[3]</em> Boyd, Danah. "Facebook's Privacy Trainwreck." Convergence: The International Journal of Research into New Media Technologies 14.1 (2008): 13-20. </span>
					<span id="ref-4"><em>[4]</em> Rubinstein, Ira, and Nathan Good. "Privacy by Design: A Counterfactual Analysis of Google and Facebook Privacy Incidents." NYU School of Law, Public Law Research Paper (2012). </span>
					</div>

			</div>

		</div>
	</div>

	<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	<script type="text/javascript" src="js/jquery.stellar.min.js"></script>
	<script type="text/javascript" src="js/waypoints.min.js"></script>
	<script type="text/javascript" src="js/jquery.easing.1.3.js"></script>
	<script type="text/javascript" src="js/scripts.js"></script>
	<style> 
		.citations span{
			display: block;
		}
		.citations span em{
			color: #00A8E1;
			font-style: normal;
			margin-right: 3px;
			display: inline-block;
		}
		p span{
			font-weight: bold;
		}
		.cit{
			font-size: 0.7em;
			position: relative;
			display: inline-block;
			top: -6px;
		}
		#breadcrumbs{
			margin-top: 10px;
		}
	</style>
	<!-- <script type="text/javascript" src="js/scripts.min.js"></script> -->
</body>
</html>
